{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "51b54fb2",
      "metadata": {
        "id": "51b54fb2"
      },
      "source": [
        "# Grad-CAM visualization with convolutional neural networks\n",
        "\n",
        "Understand datasets biaises and focus area using Grad-CAM visualization in image classification and visual regression models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d5bfb11",
      "metadata": {
        "id": "1d5bfb11"
      },
      "source": [
        "## **1. Setup and Introduction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "501a224b",
      "metadata": {
        "id": "501a224b"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install tensorflow==2.11 opencv-python requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e2a79a2",
      "metadata": {
        "id": "0e2a79a2"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import requests\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "972af93d",
      "metadata": {
        "id": "972af93d"
      },
      "source": [
        "## **2. Download Dataset and Model from Edge Impulse**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d55e8a3",
      "metadata": {
        "id": "8d55e8a3"
      },
      "outputs": [],
      "source": [
        "# Define Edge Impulse API credentials\n",
        "EI_API_KEY = \"\"  # Replace with your API Key\n",
        "\n",
        "# Adjustable parameters\n",
        "alpha=0.4\n",
        "pooling_gradients = \"mean\" # Or \"sum_abs\"\n",
        "heatmap_normalization = \"percentile\" # Or \"simple\"\n",
        "\n",
        "# Define the base URL and headers\n",
        "base_url = \"https://studio.edgeimpulse.com/v1/api\"\n",
        "headers = {\n",
        "    \"x-api-key\": EI_API_KEY,\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Retrieve project information to extract class names\n",
        "def get_project_id():\n",
        "    url = f\"{base_url}/projects\"\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "project_id = get_project_id()\n",
        "EI_PROJECT_ID = str(project_id[\"projects\"][0][\"id\"])\n",
        "print(f\"Project ID: {EI_PROJECT_ID}\")\n",
        "\n",
        "output_folder = \"output_\" + EI_PROJECT_ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ab2e1d1",
      "metadata": {
        "id": "3ab2e1d1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Retrieve the Impulse information to extract learn block ID\n",
        "def get_impulse_info(project_id):\n",
        "    url = f\"{base_url}/{project_id}/impulse\"\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "impulse_info = get_impulse_info(EI_PROJECT_ID)\n",
        "learn_block_id = impulse_info[\"impulse\"][\"learnBlocks\"][0][\"id\"]\n",
        "print(f\"Retrieved learn block ID: {learn_block_id}\")\n",
        "\n",
        "# Retrieve project information to extract class names\n",
        "def get_project_info(project_id):\n",
        "    url = f\"{base_url}/{project_id}\"\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "project_info = get_project_info(EI_PROJECT_ID)\n",
        "classes = project_info[\"dataSummaryPerCategory\"][\"training\"][\"labels\"]\n",
        "print(f\"Retrieved class names: {classes}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10ae693a",
      "metadata": {
        "id": "10ae693a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create export job for dataset\n",
        "def create_export_job(project_id):\n",
        "    url = f\"{base_url}/{project_id}/jobs/export/original\"\n",
        "    response = requests.post(url, headers=headers, json={\"uploaderFriendlyFilenames\":True,\"retainCrops\":True})\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "# Get export URL for dataset with a retry mechanism\n",
        "def get_export_url(project_id, max_retries=20, delay=10):\n",
        "    print(f\"Retrieving dataset export URL...\")\n",
        "    url = f\"{base_url}/{project_id}/export/get-url\"\n",
        "    for attempt in range(max_retries):\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            json_response = response.json()\n",
        "            if \"url\" in json_response:\n",
        "                return json_response[\"url\"]\n",
        "        print(f\"Attempt {attempt + 1} failed. URL not ready... Retrying in {delay} seconds...\")\n",
        "        if attempt == 0:\n",
        "            print(f\"Creating export job for to retrieve the dataset\")\n",
        "            create_export_job(EI_PROJECT_ID)\n",
        "        time.sleep(delay)\n",
        "    raise RuntimeError(\"Failed to retrieve export URL after multiple attempts.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa1be2f2",
      "metadata": {
        "id": "fa1be2f2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Download the dataset\n",
        "def download_dataset(url, project_name):\n",
        "    print(f\"Downloading dataset...\")\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    file_path = os.path.join(output_folder, f\"{project_name}.zip\")\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    with open(file_path, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"Downloaded dataset for project '{project_name}' to '{file_path}'\")\n",
        "    return file_path\n",
        "\n",
        "dataset_url = get_export_url(EI_PROJECT_ID)\n",
        "dataset_zip_path = download_dataset(dataset_url, \"edge_impulse_dataset\")\n",
        "\n",
        "# Extract the dataset\n",
        "dataset_dir = output_folder + \"/edge_impulse_dataset\"\n",
        "print(dataset_dir)\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "620e97f5",
      "metadata": {
        "id": "620e97f5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Download the trained model\n",
        "def get_model_url(project_id, learn_block_id):\n",
        "    url = f\"{base_url}/{project_id}/learn-data/{learn_block_id}/model/tflite-h5\"\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    file_path = output_folder + \"/model-h5.zip\"\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"Model downloaded and saved to {file_path}\")\n",
        "    return file_path\n",
        "\n",
        "# Unzip the model\n",
        "model_zip_path = get_model_url(EI_PROJECT_ID, learn_block_id)\n",
        "model_dir = output_folder + \"/edge-impulse-model\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "with zipfile.ZipFile(model_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(model_dir)\n",
        "\n",
        "# Locate the .h5 model file\n",
        "model_file_path = os.path.join(model_dir, \"model.h5\")\n",
        "if not os.path.exists(model_file_path):\n",
        "    raise FileNotFoundError(\"The extracted model does not contain the expected .h5 file.\")\n",
        "\n",
        "print(\"Model and dataset prepared successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56f2bc1b",
      "metadata": {
        "id": "56f2bc1b"
      },
      "source": [
        "## **3. Preprocessing and Model Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "531af494",
      "metadata": {
        "id": "531af494"
      },
      "outputs": [],
      "source": [
        "# Load your pre-trained model\n",
        "model = load_model(model_file_path, compile=False)\n",
        "\n",
        "# Get the input size from the model\n",
        "input_shape = model.input_shape\n",
        "input_size = (input_shape[1], input_shape[2])  # Assumes input shape is (batch_size, height, width, channels)\n",
        "\n",
        "# Get the number of output categories from the model\n",
        "output_shape = model.output_shape\n",
        "num_categories = output_shape[-1]\n",
        "\n",
        "# Assuming class names are stored in the project info\n",
        "class_names = classes if num_categories > 1 else None  # Handle regression models\n",
        "\n",
        "# Function to find the last convolutional layer dynamically\n",
        "def find_last_conv_layer(model):\n",
        "    if isinstance(model, tf.keras.Model) or isinstance(model, tf.keras.Sequential):\n",
        "        for layer in reversed(model.layers):\n",
        "            if isinstance(layer, tf.keras.layers.Conv2D):\n",
        "                return layer.name\n",
        "            if hasattr(layer, 'layers'):\n",
        "                nested_layer = find_last_conv_layer(layer)\n",
        "                if nested_layer:\n",
        "                    return nested_layer\n",
        "    raise ValueError(\"No convolutional layer found in the model.\")\n",
        "\n",
        "# Find the last convolutional layer dynamically\n",
        "last_conv_layer_name = find_last_conv_layer(model)\n",
        "\n",
        "# Grad-CAM model\n",
        "def create_grad_model(model, last_conv_layer_name):\n",
        "    if last_conv_layer_name not in [layer.name for layer in model.layers]:\n",
        "        # Check if the model contains nested models\n",
        "        for layer in model.layers:\n",
        "            if hasattr(layer, 'layers'):\n",
        "                nested_model = layer\n",
        "                break\n",
        "        else:\n",
        "            raise ValueError(\"Nested model not found, and last conv layer is invalid.\")\n",
        "        base_model = nested_model\n",
        "    else:\n",
        "        base_model = model\n",
        "\n",
        "    return Model(\n",
        "        [base_model.input],\n",
        "        [base_model.get_layer(last_conv_layer_name).output, base_model.output]\n",
        "    )\n",
        "\n",
        "grad_model = create_grad_model(model, last_conv_layer_name)\n",
        "\n",
        "# Function to preprocess an image\n",
        "def preprocess_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=input_size)  # Ensure target size matches model input size\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = img / 255.0  # Normalize\n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de13196c",
      "metadata": {
        "id": "de13196c"
      },
      "source": [
        "## **4. Grad-CAM Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56918cbb",
      "metadata": {
        "id": "56918cbb"
      },
      "outputs": [],
      "source": [
        "# Grad-CAM implementation\n",
        "@tf.function\n",
        "def make_gradcam_heatmap(img_array, grad_model, pooling_gradients, heatmap_normalization):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Compute predictions and activations\n",
        "        last_conv_layer_output, preds = grad_model(img_array)\n",
        "\n",
        "        if class_names:\n",
        "            # Classification model\n",
        "            pred_index = tf.argmax(preds[0])  # Extract scalar index of the predicted class\n",
        "            pred_index = tf.cast(pred_index, tf.int32)  # Ensure it's an integer\n",
        "            class_channel = tf.gather(preds[0], pred_index)  # Gather the value for the predicted class\n",
        "        else:\n",
        "            # Regression model (single output)\n",
        "            class_channel = preds[:, 0]  # Use the single regression output\n",
        "\n",
        "    # Calculate gradients with respect to the last convolutional layer\n",
        "    grads = tape.gradient(class_channel, last_conv_layer_output)  # <-- This line is critical\n",
        "\n",
        "    # Pool gradients based on the selected method\n",
        "    if pooling_gradients == \"mean\":\n",
        "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))  # Mean pooling\n",
        "    elif pooling_gradients == \"sum_abs\":\n",
        "        pooled_grads = tf.reduce_sum(tf.abs(grads), axis=(0, 1, 2))  # Sum of absolute gradients\n",
        "    else:\n",
        "        raise ValueError(\"Invalid pooling-gradients method. Choose 'mean' or 'sum_abs'.\")\n",
        "\n",
        "    # Create heatmap\n",
        "    last_conv_layer_output = last_conv_layer_output[0]\n",
        "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "    # Normalize the heatmap based on the selected method\n",
        "    if heatmap_normalization == \"percentile\":\n",
        "        heatmap = tf.maximum(heatmap, 0)  # ReLU to remove negatives\n",
        "        max_value = tf.reduce_max(heatmap)\n",
        "        heatmap = heatmap / max_value if max_value != 0 else heatmap  # Avoid division by zero\n",
        "    elif heatmap_normalization == \"simple\":\n",
        "        heatmap = tf.maximum(heatmap, 0) / tf.reduce_max(heatmap)  # Simple normalization\n",
        "    else:\n",
        "        raise ValueError(\"Invalid heatmap-normalization method. Choose 'percentile' or 'simple'.\")\n",
        "\n",
        "    return heatmap\n",
        "\n",
        "# Save and visualize heatmaps\n",
        "def display_and_save_gradcam(img_path, heatmap, output_dir, alpha=alpha):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, input_size)\n",
        "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    superimposed_img = heatmap * alpha + img\n",
        "\n",
        "    output_path = os.path.join(output_dir, os.path.basename(img_path))\n",
        "    cv2.imwrite(output_path, superimposed_img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Grad-CAM Visualization on Dataset**"
      ],
      "metadata": {
        "id": "sC983Mz7AMOO"
      },
      "id": "sC983Mz7AMOO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccb22797",
      "metadata": {
        "id": "ccb22797"
      },
      "outputs": [],
      "source": [
        "test_set_dir = os.path.join(dataset_dir, \"testing\")  # Replace with your test set directory\n",
        "correct_dir = output_folder + \"/gradcam/correct\"\n",
        "incorrect_dir = output_folder + \"/gradcam/incorrect\"\n",
        "os.makedirs(correct_dir, exist_ok=True)\n",
        "os.makedirs(incorrect_dir, exist_ok=True)\n",
        "\n",
        "for img_name in [f for f in os.listdir(test_set_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]:  # Filter only image files\n",
        "    img_path = os.path.join(test_set_dir, img_name)\n",
        "\n",
        "    # Dynamically determine the true class from the file name (label before the first dot)\n",
        "    true_class = img_name.split('.')[0]\n",
        "\n",
        "    img_array = preprocess_image(img_path)\n",
        "    preds = model.predict(img_array)\n",
        "\n",
        "    if class_names:\n",
        "        # Classification\n",
        "        predicted_class = class_names[np.argmax(preds)]\n",
        "        grad_model = create_grad_model(model, last_conv_layer_name)\n",
        "        heatmap = make_gradcam_heatmap(\n",
        "            img_array,\n",
        "            grad_model,\n",
        "            pooling_gradients=pooling_gradients,\n",
        "            heatmap_normalization=heatmap_normalization\n",
        "        ).numpy()\n",
        "\n",
        "        # Determine the output directory based on prediction correctness\n",
        "        if predicted_class == true_class:\n",
        "            output_dir = correct_dir\n",
        "        else:\n",
        "            output_dir = incorrect_dir\n",
        "\n",
        "    else:\n",
        "        # Regression\n",
        "        predicted_value = preds[0][0]\n",
        "        error = abs(predicted_value - float(true_class))  # Assuming filenames contain true regression values\n",
        "        heatmap = make_gradcam_heatmap(\n",
        "            img_array,\n",
        "            grad_model,\n",
        "            pooling_gradients=pooling_gradients,\n",
        "            heatmap_normalization=heatmap_normalization\n",
        "        ).numpy()\n",
        "\n",
        "        # Determine the output directory based on prediction correctness\n",
        "        threshold = 0.1  # Define a threshold for acceptable error\n",
        "        if error <= threshold:\n",
        "            output_dir = correct_dir\n",
        "        else:\n",
        "            output_dir = incorrect_dir\n",
        "\n",
        "    # Save the Grad-CAM image\n",
        "    display_and_save_gradcam(img_path, heatmap, output_dir)\n",
        "\n",
        "print(\"Grad-CAM visualizations completed.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}